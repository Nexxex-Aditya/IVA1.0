# MoE Transformer Configuration
model:
  d_model: 1024
  n_layers: 16
  n_heads: 16
  vocab_size: 32000
  seq_len: 4096
  rope_theta: 100000.0
  attn_impl: "triton"
  norm_impl: "triton"
  moe_enable: true
  hrm_enable: true
  tie_embeddings: true
  sliding_window: null

# MoE config
moe:
  n_experts: 4
  top_k: 2
  capacity_factor: 1.25
  dropless: false
  aux_loss_coef: 0.01
  expert_parallel: false
  router:
    temp: 1.0
    jitter: 0.1
    noise_std: 0.1
  expert:
    hidden_mult: 4
    activation: "swiglu"
    dropout: 0.1

hrm:
  max_steps: 8
  halt_penalty: 0.1
  sink_strength_init: 0.5
  temp_bounds: [0.1, 2.0]
  z_h_init: 1.0
  z_l_init: 0.1

attention:
  dropout: 0.1
  use_flash_attn: false
  causal: true
  rope_theta: 100000.0

norm:
  eps: 1e-6
  elementwise_affine: true

generation:
  max_length: 2048
  temperature: 0.8
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  do_sample: true
  pad_token_id: 0
  eos_token_id: 2
