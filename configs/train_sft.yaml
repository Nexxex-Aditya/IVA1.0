# Supervised Fine Tuning Configuration
training:
  micro_batch: 4
  grad_accum: 8
  effective_batch_size: 32
  lr: 2.5e-4
  weight_decay: 0.1
  epochs: 1
  max_steps: 1000
  
  optimizer: "adamw"   # optimization
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1e-8
  lr_scheduler: "cosine"
  warmup_steps: 100
  warmup_ratio: 0.1
  
  amp: "bf16"  # fp16 | bf16 | no
  grad_clip: 1.0
  
  activation_checkpointing: true
  gradient_checkpointing: true
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  
  log_interval: 10
  eval_interval: 100
  save_interval: 500
  max_checkpoints: 3
  
  seed: 42

data:
  dataset: "tiny-reasoning-mix"
  max_length: 2048
  packing: true
  attention_sink: true
  sink_tokens: 4
  
  batch_size: 32
  shuffle: true
  drop_last: true
  
  tokenizer: "Qwen/Qwen2.5-0.5B"
  add_special_tokens: true
  padding: "max_length"
  truncation: true

model:
  pretrained_model: "Qwen/Qwen2.5-0.5B"
  trust_remote_code: false
  
  d_model: 1024
  n_layers: 16
  n_heads: 16
  vocab_size: 32000
  seq_len: 4096
  
  hrm_enable: true
  moe_enable: false
  attn_impl: "triton"
  norm_impl: "triton"

loss:
  ce_weight: 1.0
  
  kd_enable: false
  kd_weight: 0.5
  kd_temperature: 3.0
  
  hrm_loss_weight: 1.0
  halt_penalty: 0.1
  sink_loss_weight: 0.1
  
  moe_aux_loss_weight: 0.01
  router_kl_weight: 0.01

evaluation:
  metrics:
    - "perplexity"
    - "accuracy"
    - "halt_rate"
    - "tokens_per_sec"
    - "vram_gb"
  
  eval_datasets:
    - "tiny-reasoning-mix"
  
  eval_batch_size: 16
  eval_max_length: 1024
  eval_do_sample: false
  eval_temperature: 1.0

logging:
  tensorboard: true
  jsonl: true
  console: true
  
  log_dir: "./logs"
  tensorboard_dir: "./logs/tensorboard"
  
  log_level: "INFO"
  
  log_metrics:
    - "loss"
    - "learning_rate"
    - "perplexity"
    - "halt_rate"
    - "tokens_per_sec"
    - "vram_gb"
    - "grad_norm"
